{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Subbaroa/Fmml-assignments/blob/main/Module_01_Lab_04_LinearAlgebra.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mU9LZjQxiQT2"
      },
      "source": [
        "# Transforming data using linear algebra\n",
        "\n",
        "FMML Module 1, Lab 4<br>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KBe4ZA32UTbv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.datasets import mnist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "7mqG-dafVpya"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "\n",
        "from IPython.display import Latex as lt\n",
        "#Plotting functions\n",
        "#(You DON'T need to understand how these functions)\n",
        "\n",
        "# function to plot a grid\n",
        "def plotGrid(transform, unit, linestyle = ':', fig=None, ax=None):\n",
        "  lim1 = -100\n",
        "  lim2 = 100\n",
        "  def mat2xy(start, end):\n",
        "    if len(start.shape)==1:\n",
        "      start = np.expand_dims(start,0)\n",
        "      end = np.expand_dims(end,0)\n",
        "    nan = np.ones(len(start))*np.nan\n",
        "    x = np.stack((start[:,0], end[:,0], nan)).T.reshape(-1)\n",
        "    y = np.stack((start[:,1], end[:,1], nan)).T.reshape(-1)\n",
        "    return x, y\n",
        "\n",
        "  def parallellines(axis, addend, lines, unit):\n",
        "    addend = np.repeat(np.expand_dims(addend,0), lines*2, 0)\n",
        "    unit = np.expand_dims(np.arange(-lines, lines)*unit,1)\n",
        "    unit = unit-lines\n",
        "    addend = addend*unit\n",
        "    lines = np.expand_dims(axis,0) + addend\n",
        "    return np.concatenate((lines, lines*-1))\n",
        "\n",
        "  if fig is None:\n",
        "    fig, ax = plt.subplots(figsize=(10,10))\n",
        "  transform = transform.astype(np.float)\n",
        "  xaxis = transform[0]\n",
        "  yaxis = transform[1]\n",
        "\n",
        "  # plot lines parallel to the x axis\n",
        "  lines1= parallellines(xaxis*lim1, yaxis, 100,unit )\n",
        "  lines2 = parallellines(xaxis*lim2, yaxis, 100,unit )\n",
        "  x,y = mat2xy(lines1, lines2)\n",
        "  plt.plot(x,y, linestyle+'k', linewidth=0.5)\n",
        "  # plot x axis\n",
        "  x,y = mat2xy(xaxis*lim1, xaxis*lim2)\n",
        "  plt.plot(x,y,linestyle, color = '#440077')\n",
        "\n",
        "  # plot  lines parallel to the y axis\n",
        "  lines1= parallellines(yaxis*lim1, xaxis, 100,unit)\n",
        "  lines2 = parallellines(yaxis*lim2, xaxis, 100,unit)\n",
        "  x,y = mat2xy(lines1, lines2)\n",
        "  plt.plot(x,y, linestyle+'k', linewidth=0.5)\n",
        "  # plot y axis\n",
        "  x,y = mat2xy(yaxis*lim1, yaxis*lim2)\n",
        "  plt.plot(x,y,linestyle, color= '#aa5500')\n",
        "\n",
        "  return fig, ax\n",
        "\n",
        "def plotData(X, y, xlabel = 'hole', ylabel = 'bound', fig=None, ax = None):\n",
        "\n",
        "  if fig is None:\n",
        "    fig, ax = plt.subplots()\n",
        "  for ii in range(nclasses):\n",
        "    plt.scatter(X[y==ii,0], X[y==ii, 1])\n",
        "  plt.legend([str(i) for i in range(nclasses)])\n",
        "  plt.xlabel(xlabel)\n",
        "  plt.ylabel(ylabel)\n",
        "  lim2 = X.max()\n",
        "  lim1 = X.min()\n",
        "  add = abs(lim1-lim2)/5\n",
        "  return fig, ax"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d5dmEXxaktp"
      },
      "source": [
        "# Matrix transformations on data\n",
        "\n",
        "A 2D coordinate system is defined by its basis vectors, i and j. In the standard coordinate system (Let us call it T0), the basis vectors are\n",
        "\n",
        "$$\\begin{equation}\n",
        "i = \\left\\{  \\begin{aligned}1 \\\\ 0 \\end{aligned} \\right\\}\n",
        "\\end{equation}$$\n",
        "and\n",
        "$$\\begin{equation} j = \\left\\{ \\begin{aligned} 0 \\\\ 1\\end{aligned} \\right\\} \\end{equation}$$\n",
        "\n",
        "We can use any two vectors as basis vectors for a new coordinate system as long as they are not colinear. For example, let us call this new coordinate system T1:\n",
        "\n",
        "$$\\begin{equation}\n",
        "i = \\left\\{  \\begin{aligned}1 \\\\ -1 \\end{aligned} \\right\\}\n",
        "\\end{equation}$$\n",
        "and\n",
        "$$\\begin{equation} j = \\left\\{ \\begin{aligned} 0 \\\\ 2 \\end{aligned} \\right\\} \\end{equation}$$\n",
        "\n",
        "Suppose we have a point [a,b] in the T1 coordinate system. Its representation in the standard system T0 can be obtained by the following matrix multiplication:\n",
        "\n",
        "$$ \\begin{equation}\n",
        "\\left\\{  \\begin{aligned}a' \\\\ b' \\end{aligned} \\right\\} =\n",
        "\\left\\{  \\begin{aligned}&1 & 0 \\\\ -&1 & 2 \\end{aligned} \\right\\}\n",
        "\\left\\{  \\begin{aligned}a \\\\ b \\end{aligned} \\right\\}\n",
        "\\end{equation}$$\n",
        "where the columns of the matrix are the basis vectors of T1.\n",
        "\n",
        "Let us see this in action:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K8SCFKqfbM-l"
      },
      "outputs": [],
      "source": [
        "T0 = np.array([[1,0],[0,1]])\n",
        "T1 = np.array([[1,0], [-1,2]])\n",
        "\n",
        "data1 = np.array([5,4]) # the data in T1 coordinate system\n",
        "data0 = np.matmul(T1, data1) # the data in T0 coordinate system\n",
        "\n",
        "print('Data in T0 = ', data0)\n",
        "print('Data in T1 = ', data1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIfEWZ1RQeve"
      },
      "source": [
        "We can visualize this below. T0 is shown with dotted lines and T1 is shown with solid lines."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jSuTaeSDQoYK"
      },
      "outputs": [],
      "source": [
        "fig, ax = plotGrid(T1.T, 1,'-') # custom plotting function, no need to understand this\n",
        "plotGrid(T0.T, 1, fig=fig, ax=ax) # custom plotting function, no need to understand this\n",
        "plt.scatter(data0[0], data0[1])\n",
        "ax.set_xlim(-10,10)\n",
        "ax.set_ylim(-10,10)\n",
        "ax.set_xticks([]);\n",
        "ax.set_yticks([]);\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUb3oYWIgdya"
      },
      "source": [
        "Look at the coordinates of the blue dot. In T0 (dotted lines), the position is [5,3] where it is [5,4] in T1. Feel free to experiment with different data points and coordinate systems.\n",
        "\n",
        "Remember that we can achieve the same thing by post-multiplying the transpose of the transformation matrix to the data. This will come in handy when transforming multiple data points at once:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SD4pqOMndf-4"
      },
      "outputs": [],
      "source": [
        "data0_a = np.matmul(T1, data1)\n",
        "data0_b = np.matmul(data1, T1.T)\n",
        "print(data0_a)\n",
        "print(data0_b)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-Kui3dSESPm"
      },
      "source": [
        "Why is transforming data useful? Data transformations cause the distance between data points to change. This will affect distance-based algorithms such as nearest neighbour"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nE0NbpYIC9ou"
      },
      "outputs": [],
      "source": [
        "# let us define 3 points in T1\n",
        "A1 = np.array([3,3])\n",
        "B1 = np.array([2,-5])\n",
        "C1 = np.array([1,-1])\n",
        "\n",
        "# the corresponding points in T0:\n",
        "A0 = np.matmul(T1, A1)\n",
        "B0 = np.matmul(T1, B1)\n",
        "C0 = np.matmul(T1, C1)\n",
        "\n",
        "# function to calculate Euclidean distance:\n",
        "def dist(a, b):\n",
        "  diff = a-b\n",
        "  sq = diff*diff\n",
        "  return np.sqrt(sq.sum())\n",
        "\n",
        "# distance between the points in T1\n",
        "print('Distance between A and B in T1 = ', dist(A1, B1))\n",
        "print('Distance between B and C in T1 = ', dist(B1, C1))\n",
        "print('Distance between A and C in T1 = ', dist(A1, C1))\n",
        "\n",
        "print('')\n",
        "# distnace between the points in T0\n",
        "print('Distance between A and B in T0 = ', dist(A0, B0))\n",
        "print('Distance between B and C in T0 = ', dist(B0, C0))\n",
        "print('Distance between A and C in T0 = ', dist(A0, C0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YE95JMniUtDm"
      },
      "source": [
        "We see that in T1, B and C are the closest whereas in T0, A and C are the closest. These kinds of changes will affect the predictions returned by the nearest neighbour algorithm."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PFbpTSBdV29C"
      },
      "source": [
        "# Transformations on MNIST\n",
        "\n",
        "Let us experiment with a subset of the MNIST dataset. We will extract two features from the database for our experiment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMvUzC2GUawS"
      },
      "source": [
        "Functions for nearest neighbour, accuracy and feature extraction. (from previous labs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "22ayl0sViF5-"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "\n",
        "def NN1(traindata, trainlabel, query):\n",
        "  diff  = traindata - query  # find the difference between features. Numpy automatically takes care of the size here\n",
        "  sq = diff*diff # square the differences\n",
        "  dist = sq.sum(1) # add up the squares\n",
        "  label = trainlabel[np.argmin(dist)] # our predicted label is the label of the training data which has the least distance from the query\n",
        "  return label\n",
        "\n",
        "def NN(traindata, trainlabel, testdata):\n",
        "  # we will run nearest neighbour for each sample in the test data\n",
        "  # and collect the predicted classes in an array using list comprehension\n",
        "  predlabel = np.array([NN1(traindata, trainlabel, i) for i in testdata])\n",
        "  return predlabel\n",
        "\n",
        "def Accuracy(gtlabel, predlabel):\n",
        "  assert len(gtlabel)==len(predlabel), \"Length of the groundtruth labels and predicted labels should be the same\"\n",
        "  correct = (gtlabel==predlabel).sum() # count the number of times the groundtruth label is equal to the predicted label.\n",
        "  return correct/len(gtlabel)\n",
        "\n",
        "def cumArray(img):\n",
        "  img2 = img.copy()\n",
        "  for ii in range(1, img2.shape[1]):\n",
        "    img2[ii,:] = img2[ii,:] + img2[ii-1,:]  # for every row, add up all the rows above it.\n",
        "  img2 = img2>0\n",
        "  return img2\n",
        "\n",
        "def getHolePixels(img):\n",
        "  im1 = cumArray(img)\n",
        "  im2 = np.rot90(cumArray(np.rot90(img)), 3) # rotate and cumulate it again for differnt direction\n",
        "  im3 = np.rot90(cumArray(np.rot90(img, 2)), 2)\n",
        "  im4 = np.rot90(cumArray(np.rot90(img, 3)), 1)\n",
        "  hull =  im1 & im2 & im3 & im4 # this will create a binary image with all the holes filled in.\n",
        "  hole = hull & ~ (img>0) # remove the original digit to leave behind the holes\n",
        "  return hole\n",
        "\n",
        "def getHullPixels(img):\n",
        "  im1 = cumArray(img)\n",
        "  im2 = np.rot90(cumArray(np.rot90(img)), 3) # rotate and cumulate it again for differnt direction\n",
        "  im3 = np.rot90(cumArray(np.rot90(img, 2)), 2)\n",
        "  im4 = np.rot90(cumArray(np.rot90(img, 3)), 1)\n",
        "  hull =  im1 & im2 & im3 & im4 # this will create a binary image with all the holes filled in.\n",
        "  return hull\n",
        "\n",
        "def minus(a, b):\n",
        "  return a & ~ b\n",
        "\n",
        "def getBoundaryPixels(img):\n",
        "  img = img.copy()>0  # binarize the image\n",
        "  rshift = np.roll(img, 1, 1)\n",
        "  lshift = np.roll(img, -1 ,1)\n",
        "  ushift = np.roll(img, -1, 0)\n",
        "  dshift = np.roll(img, 1, 0)\n",
        "  boundary = minus(img, rshift) | minus(img, lshift) | minus(img, ushift) | minus(img, dshift)\n",
        "  return boundary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RzF-9mGxUkWC"
      },
      "source": [
        "Get the MNIST dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bHz5BVmLUjzb"
      },
      "outputs": [],
      "source": [
        "#loading the dataset\n",
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
        "train_X = train_X/255\n",
        "test_X = test_X/255\n",
        "\n",
        "nclasses = 4\n",
        "\n",
        "# get only for the first 4 classes\n",
        "train_X = train_X[train_y<nclasses]\n",
        "train_y = train_y[train_y<nclasses]\n",
        "test_X = test_X[test_y<nclasses]\n",
        "test_y = test_y[test_y<nclasses]\n",
        "\n",
        "train_X = train_X[::100].copy() # We are only taking a subset of the training set\n",
        "train_y = train_y[::100].copy() # do the same to the labels\n",
        "test_X = test_X[::100].copy() # taking a subset of the test set. This code takes every 500th sample\n",
        "test_y = test_y[::100].copy()\n",
        "\n",
        "# get all the features\n",
        "train_hole = np.array([getHolePixels(i).sum() for i in train_X])\n",
        "test_hole = np.array([getHolePixels(i).sum() for i in test_X])\n",
        "train_bound = np.array([getBoundaryPixels(i).sum() for i in train_X])\n",
        "test_bound = np.array([getBoundaryPixels(i).sum() for i in test_X])\n",
        "# train_hull = np.array([getHullPixels(i).sum() for i in train_X])\n",
        "# test_hull = np.array([getHullPixels(i).sum() for i in test_X])\n",
        "# train_sum = np.sum(train_X, (1,2))/(28*28)\n",
        "# test_sum = np.sum(test_X, (1,2))/(28*28)\n",
        "\n",
        "# create the train and test set by combining the appropriate features\n",
        "train_feats = np.vstack((train_hole,train_bound)).transpose()\n",
        "test_feats = np.vstack((test_hole, test_bound)).transpose()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7qGVrlnQCUy"
      },
      "source": [
        "Let us plot the samples and see what they look like:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "saRzHfi9QAGd"
      },
      "outputs": [],
      "source": [
        "# fix limits of x and y axis so that we can see what is going on\n",
        "xlim=[-100,300]\n",
        "ylim=[-100,300]\n",
        "fig, ax = plotData(train_feats, train_y)\n",
        "ax.set_xlim(xlim)\n",
        "ax.set_ylim(ylim)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCb4DikQP1ck"
      },
      "source": [
        "Check the baseline accuracy on the test set:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxVr6bd9PzlI"
      },
      "outputs": [],
      "source": [
        "test_pred = NN(train_feats, train_y, test_feats)\n",
        "acc = Accuracy(test_y, test_pred)\n",
        "print('Baseline accuracy = ', acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nl8Noo8pZRek"
      },
      "source": [
        "Let us try transforming the features and checking their accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dUPWRsEZKwo"
      },
      "outputs": [],
      "source": [
        "transform = np.array([[0.5,-0.5],[0,2.5]])\n",
        "\n",
        "train_feats_t = np.matmul(train_feats, transform)\n",
        "test_feats_t = np.matmul(test_feats, transform)  # whatever transform we are applying to the training set should be applied to the test set also"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqMBUAfqIUcC"
      },
      "outputs": [],
      "source": [
        "print(transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRH4VckHZaWv"
      },
      "outputs": [],
      "source": [
        "fig, ax = plotData(train_feats_t, train_y)\n",
        "ax.set_xlim(xlim)\n",
        "ax.set_ylim(ylim)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l9fknczfZdYF"
      },
      "outputs": [],
      "source": [
        "test_pred = NN(train_feats_t, train_y, test_feats_t)\n",
        "acc = Accuracy(test_y, test_pred)\n",
        "print('Accuracy after transform = ', acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OFBQlAnNZ3on"
      },
      "source": [
        "## Questions:\n",
        "1. Experiment with different transformation matrices and check the accuracy\n",
        "2. Will the same transform used for these two features also work for other features?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***1st Question Answer***\n",
        "\n",
        "\n",
        "Certainly, experimenting with different transformation matrices can help you understand how different linear transformations impact the accuracy of your machine learning model. Here's how you can perform this experiment:\n",
        "\n",
        "***Define a List of Transformation Matrices***: Create a list of transformation matrices that you want to experiment with. Each matrix represents a different linear transformation."
      ],
      "metadata": {
        "id": "pjUr-ZPwtfO_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example transformation matrices\n",
        "transformations = [np.array([[1, 0], [0, 1]]),  # Identity matrix (no transformation)\n",
        "                   np.array([[0.5, -0.5], [0, 2.5]]),  # Example transformation 1\n",
        "                   np.array([[2, 0], [0, 0.5]])]  # Example transformation 2"
      ],
      "metadata": {
        "id": "O_182wONvNkG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Loop Through Transformation Matrices***: Iterate through the list of transformation matrices, applying each transformation to your feature vectors, and calculating the accuracy for each transformation."
      ],
      "metadata": {
        "id": "HylifMoLvSMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for transform_matrix in transformations:\n",
        "    # Apply the current transformation to training and test features\n",
        "    train_feats_transformed = np.matmul(train_feats, transform_matrix)\n",
        "    test_feats_transformed = np.matmul(test_feats, transform_matrix)\n",
        "\n",
        "    # Make predictions on the transformed test data\n",
        "    test_pred = NN(train_feats_transformed, train_y, test_feats_transformed)\n",
        "\n",
        "    # Calculate accuracy on the transformed data\n",
        "    acc = Accuracy(test_y, test_pred)\n",
        "\n",
        "    # Print the accuracy and the description of the transformation (optional)\n",
        "    print(f'Accuracy after transformation:\\n{transform_matrix}\\nAccuracy = {acc}\\n')"
      ],
      "metadata": {
        "id": "m1c6jPqnvXtl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*You loop through each transformation matrix in the transformations list.\n",
        "\n",
        "*For each transformation, you apply it to the training and test features using matrix multiplication (np.matmul).\n",
        "\n",
        "*You then make predictions on the transformed test data and calculate the accuracy.\n",
        "\n",
        "*Finally, you print the accuracy along with the description of the transformation (which you can customize).\n",
        "\n",
        "By running this code, you can experiment with different linear transformations and observe how they affect the accuracy of your machine learning model. This allows you to gain insights into the impact of feature transformations on your classifier's performance."
      ],
      "metadata": {
        "id": "_lWsPmFrvcfi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***2nd Question Answer***\n",
        "\n",
        "\n",
        "In machine learning and linear algebra, the applicability of the same transformation to different features depends on the nature of the features and the problem you're trying to solve. Here are some considerations:\n",
        "\n",
        "1. **Similarity of Features**: If the features represent similar aspects of the data, such as different measurements or characteristics of the same underlying phenomenon, then the same transformation may be applicable. For example, if you have two features representing different physical measurements and you find that a certain linear transformation improves model performance, it's possible that the same transformation could be beneficial for other similar features.\n",
        "\n",
        "2. **Domain Knowledge**: Domain knowledge plays a crucial role. Understanding the meaning and relevance of the features can guide the choice of transformations. A transformation that makes sense for one type of feature may not be suitable for others.\n",
        "\n",
        "3. **Feature Scaling**: Some transformations are scale-dependent. For example, standardization (mean centering and scaling by the standard deviation) can be applied to different features as long as the scaling factors are relevant to each feature. However, the same magnitude of transformation may not be suitable for features with vastly different scales.\n",
        "\n",
        "4. **Feature Independence**: If features are independent or have known relationships, a transformation that works for one feature might not work for another. In such cases, feature engineering may involve considering interactions or transformations tailored to specific features.\n",
        "\n",
        "5. **Experimental Validation**: It's important to empirically validate the impact of a transformation on different features. What works well for one set of features may not work as effectively for others. Experimentation and cross-validation are key steps to determine the most appropriate transformations.\n",
        "\n",
        "6. **Dimensionality Reduction**: Principal Component Analysis (PCA) is a technique that finds linear combinations (transformations) of features that capture the most variance in the data. PCA-based transformations are applied to all features collectively, not to individual features separately.\n",
        "\n",
        "In summary, while the same transformation can be applied to different features in some cases, it's not a one-size-fits-all approach. The choice of transformation should consider the nature of the features, their relationships, and the specific problem you're trying to solve. Experimentation and domain knowledge are essential for making informed decisions about feature transformations in machine learning."
      ],
      "metadata": {
        "id": "fEfAB_hOvi_0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36TOA47xak20"
      },
      "source": [
        "# Data normalization\n",
        "\n",
        "Sometimes the features of our data have vastly different scales. This will cause the learning algorithm to give more importance to certain features, reducing its performance. Data normalization is a method in which we transform the features so that they have similar scales.\n",
        "\n",
        "Three commonly used feature scaling techniques are rescaling, mean normalization and z-score normalization. Here, we will talk about the simplest one: rescaling.\n",
        "\n",
        "$$\\begin{equation}\n",
        "x' = \\frac {x -min(x)} { max(x) - min(x)}\n",
        "\\end{equation}$$\n",
        "\n",
        "\n",
        "\n",
        "For more information, see [this page](https://towardsdatascience.com/data-normalization-in-machine-learning-395fdec69d02)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ni19QKDLZzeo"
      },
      "outputs": [],
      "source": [
        "def rescale(data):\n",
        "  return (data - data.min())/(data.max() - data.min())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8k83ZMtKeZrQ"
      },
      "source": [
        "We have to apply the rescaling to each feature individually. Also remember to apply the same transform we are using on the train set to the test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1JLOPwhvehpR"
      },
      "outputs": [],
      "source": [
        "train_feats_rescaled_x = rescale(train_feats[:,0])\n",
        "train_feats_rescaled_y = rescale(train_feats[:,1])\n",
        "train_feats_rescaled = np.stack((train_feats_rescaled_x, train_feats_rescaled_y),1)\n",
        "\n",
        "test_feats_rescaled_x = rescale(test_feats[:,0])\n",
        "test_feats_rescaled_y = rescale(test_feats[:,1])\n",
        "test_feats_rescaled = np.stack((test_feats_rescaled_x, test_feats_rescaled_y),1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaZVy9vxfKwX"
      },
      "source": [
        "Let us plot the rescaled features:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXyatpwZevOH"
      },
      "outputs": [],
      "source": [
        "fig, ax = plotData(train_feats_rescaled, train_y)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjBgxE6zglsL"
      },
      "source": [
        "This type of rescaling makes all the features between 0 and 1.\n",
        "\n",
        "Let us calculate the accuracy obtained by this transform:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKQnsj-KgNZc"
      },
      "outputs": [],
      "source": [
        "test_pred = NN(train_feats_rescaled, train_y, test_feats_rescaled)\n",
        "acc = Accuracy(test_y, test_pred)\n",
        "print('Accuracy after transform = ', acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qYX90Gqg-jO"
      },
      "source": [
        "All 2D linear transformations can be repreented by a transformation matrix. So what is the matrix associated with the rescaling function? Actually, we cannot represent rescaling with a matrix multiplication, because it is not a linear transform. Rescaling involves shifting the origin of the data, which is not allowed under linear transformations.\n",
        "\n",
        "We can represent rescaling as a matrix multiplication followed by a vector addition. Let our first feature vector be called X and second feature vector be called Y. Suppose we want to rescale a data point [a,b]\n",
        "\n",
        "$$ \\begin{equation}\n",
        " \\left\\{  \\begin{aligned}a' \\\\ b' \\end{aligned} \\right\\} =\n",
        " \\left\\{  \\begin{aligned} \\frac{a - min(X)}{max(X) - min(X)} \\\\ \\frac{b - min(Y)}{max(Y) - min(Y)} \\end{aligned} \\right\\} =\n",
        " \\left\\{  \\begin{aligned}&\\frac{1}{max(X)-min(X)} &0\\\\ &0 &\\frac{1}{max(Y)-min(Y)} \\end{aligned}\n",
        " \\right\\}\\left\\{  \\begin{aligned}a \\\\ b \\end{aligned} \\right\\} +\n",
        " \\left\\{  \\begin{aligned} \\frac{ -min(X)}{max(X) - min(X)} \\\\ \\frac{-min(Y)}{max(Y) - min(Y)} \\end{aligned} \\right\\}\n",
        "\\end{equation}$$\n",
        "\n",
        "You can verify this yourself if you wish, though it is not necessary.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}